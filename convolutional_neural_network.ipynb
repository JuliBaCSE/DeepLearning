{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_neural_network_Dog_vs_Cat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliBaCSE/DeepLearning/blob/CNN/convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network Dog vs Cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCV30xyVhFbE"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIleuCAjoFD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "87d6fa73-aa5d-4d9a-ee29-3794024a75c2"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing\n",
        "As input we have 4000 dog and 4000 cat images for training and for testing we have 1000 of each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set\n",
        "\n",
        "First we are going to apply transformations on our training set to avoid overfitting (high accuracy on training set and low accuracy on test set). \n",
        "\n",
        "! do not on test set !\n",
        "\n",
        "For Transformations we are doing some simple geometric transformations (zoom, rotation, flip,...). This will give us new images (augmented) and therefore the CNN does not overlearn.\n",
        "\n",
        "to do this we use from keras the imagedatagenerator class. this allow us real time data augmentation.\n",
        "\n",
        "in our example we use the shear, zoom, flip\n",
        "\n",
        "with rescale we get all pixel values between 0 an 1 \n",
        "\n",
        "In the second part we import the images from the training set and resize (to target size) to reduce computations. Batch size tells how many images are in one batch. Since the outcome is either cat or dog (binary) we use class mode binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0koUcJMJpEBD"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set\n",
        "\n",
        "since we want to keep our test images as they are to avoid information leackage we dont apply the transformations here. !!But we still have to rescale the pixel values to have later the same scaling in the CNN\n",
        "\n",
        "finally we import our testset images to our notbook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH4WzfOhpKc3"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN\n",
        "as for the ann we again use the sequential class to creat an ann with sequence of layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAUt4UMPlhLS"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution\n",
        "Now we use instead of a dense layer (compare ann) we use the Conv2D class. \n",
        "\n",
        "Here we specify the number of feature detectures with filters = ... \n",
        "\n",
        "the size of those feature detectures in kernel_size (a feature detecture is always quadratic here squared array (size x size)).\n",
        "\n",
        "the rectifier activation function \n",
        "\n",
        "\n",
        "\n",
        "and the input shape for the very first layer\n",
        "\n",
        "here we specify the size (since we reshaped to 64 x 64 pixel we plug this in)\n",
        "\n",
        "and since use RGB pictures we use 3 in the end \n",
        "\n",
        "!! for black and white we would use 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzPrMckl-hV"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling\n",
        "\n",
        "in pooling we use the maxpooling which is often the better choice as seen in the paper of Dominik Scherer Uni Bonn\n",
        "\n",
        "Therefore we add a obj from the layer class MaxPool2D:\n",
        "\n",
        "pool_size = frame were we take out the max (here 2 x 2)  \n",
        "\n",
        "strides = is number of pixels the frame is shifted to the right/bottom. (often stride 2 is used)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncpqPl69mOac"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer\n",
        "\n",
        "now the dont need the input shape paramter since it isnt the first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-FZjn_m8gk"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening\n",
        "\n",
        "finally we want to flatten into a 1D array to use it as input in our ann"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZeOGCvnNZn"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection\n",
        "\n",
        "for the full connection we add a new dense layer that gets connected to our flattened layer. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GtmUlLd26Nq"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer\n",
        "\n",
        "since we have either dog or cat we only need one unit as output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_Zj1Mc3Ko_"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALksrNQpUlJ"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUj1W4PJptta"
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction\n",
        "\n",
        "since we only imported something specific from the imagre lib we now need to import image again for the tools that we need now.\n",
        "\n",
        "first we will load the image from our folder that we want to use for the test and resize it to the size we used in our model for training/test\n",
        "\n",
        "since our predict methods need a 2D array as input we have to convert our test image from PIL-fromat to an array\n",
        "\n",
        "since we trained with batches (each contains 32 pictures as we defined) we have a extra dimension. therefore we have to add an extra dimension that corresponds to the batch (therefore we use numpy). axis = 0 means that we add that extra dimension as the first dimension. \n",
        "\n",
        "Now have the right fromat to input it into the predict method\n",
        "\n",
        "the result will contain 0 and 1 \n",
        "\n",
        "training_set.class_indices will give us the indices for dog and cat (which is 1 and which is 0)\n",
        "\n",
        "we first access the batch with [0] and then access the first element inside this batch [0] which contains the prediction\n",
        "\n",
        "Since dog is 1 and cat is 0 we can use an if statement that tell us if it is a dog or cat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSiWEJY1BPB"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED9KB3I54c1i"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
